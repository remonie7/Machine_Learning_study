{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572.4168027209098\n",
      "10369.467107156093\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "####데이터 읽어오기####\n",
    "#######################\n",
    "train_X = pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_trainX.csv\",encoding='euc-kr')\n",
    "train_y= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_trainy.csv\",encoding='euc-kr')\n",
    "test_X= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_testX.csv\",encoding='euc-kr')\n",
    "sub= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/subExample.csv\",encoding='euc-kr')\n",
    "#print(train_X.head(3))\n",
    "#print(train_y.head(3))\n",
    "#print(test_X.head(3))\n",
    "#print(sub.head(3))\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "####결측치수, 데이터타입, 연속형변수 분포 확인####\n",
    "##################################################\n",
    "#print(train_X.info())\n",
    "#print(test_X.info())\n",
    "#print(train_y.info())\n",
    "#print(sub.info())\n",
    "#print(train_X.describe())\n",
    "#print(test_X.describe())\n",
    "\n",
    "#corr_df=pd.merge(train_X, train_y, on='id')\n",
    "#corr_df=corr_df[['거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', \n",
    "#                 '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속','교통량']]\n",
    "#print(corr_df.corr()['교통량'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 이상치 기준\n",
    "total=pd.concat([train_X, test_X], axis=0)\n",
    "#print(total.describe())\n",
    "total_num=total[['거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', \n",
    "                 '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속']]\n",
    "total_str=total[['일자', '시도명', '읍면동명']]\n",
    "lower_outlier=list(map(lambda x: np.percentile(total_num[x],25)-((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "upper_outlier=list(map(lambda x: np.percentile(total_num[x],75)+((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "#print(lower_outlier)\n",
    "#print(upper_outlier)\n",
    "# 정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler=StandardScaler()\n",
    "standardscaler.fit(total_num)\n",
    "\n",
    "\n",
    "# 범주형변수 train-test비교 - 차이없음\n",
    "#print(set(train_X['시도명'])-set(test_X['시도명']))\n",
    "#print(set(test_X['시도명'])-set(train_X['시도명']))\n",
    "#print(set(train_X['읍면동명'])-set(test_X['읍면동명']))\n",
    "#print(set(test_X['읍면동명'])-set(train_X['읍면동명']))\n",
    "#print(train_X['시도명'].value_counts())\n",
    "#print(test_X['시도명'].value_counts())\n",
    "#print(train_X['읍면동명'].value_counts())\n",
    "#print(test_X['읍면동명'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(xdata):\n",
    "    x_num=xdata[['거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', \n",
    "                 '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속']]\n",
    "    x_str=xdata[['일자', '시도명', '읍면동명']]\n",
    "    \n",
    "    #for i in range(len(x_num.columns)):\n",
    "    #    x_num[x_num.columns[i]]=[lower_outlier[i] if x<lower_outlier[i] else upper_outlier[i] if x>upper_outlier[i] else x for x in x_num[x_num.columns[i]]]\n",
    "    #print(x_num.describe())\n",
    "    x_num=pd.DataFrame(standardscaler.transform(x_num), columns=x_num.columns)\n",
    "    #print(x_num.describe())\n",
    "    x_str=pd.get_dummies(x_str, columns=['시도명','읍면동명'])\n",
    "    x_str['일자']=pd.to_datetime(x_str['일자'])    \n",
    "    x_str['연도']=x_str['일자'].dt.year\n",
    "    x_str['월']=x_str['일자'].dt.month\n",
    "    x_str['일']=x_str['일자'].dt.day\n",
    "    x_str['weekend']=x_str['일자'].dt.weekday\n",
    "    x_str.drop('일자', axis=1, inplace=True)\n",
    "    #print(x_str)\n",
    "    x_data=pd.concat([x_num, x_str], axis=1)\n",
    "    #x_data.drop(['평균 속도', '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속'], axis=1, inplace=True)\n",
    "    return x_data\n",
    "    \n",
    "x_data=preprocessing(train_X)\n",
    "corr_df=pd.concat([x_data, train_y], axis=1)\n",
    "corr_df=corr_df.drop('id', axis=1)\n",
    "#print(corr_df.corr()['교통량'].)\n",
    "corr_prep=corr_df.corr()['교통량']\n",
    "#print(corr_prep[abs(corr_prep)<0.1].index)\n",
    "#print(corr_prep[abs(corr_prep)>=0.3])\n",
    "corr_del_index=corr_prep[abs(corr_prep)<0.1].index\n",
    "x_data.drop(corr_del_index, axis=1, inplace=True)\n",
    "\n",
    "#corr_inc_index=['거주인구','근무인구','총 유동인구','평균 속도','시도명_서귀포시','시도명_제주시','읍면동명_노형동','읍면동명_도두동','읍면동명_화북동']\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "#pca=PCA()\n",
    "#pca.fit(x_data)\n",
    "#print(pca.explained_variance_ratio_.cumsum())\n",
    "pca=PCA(n_components=20)\n",
    "pca.fit(x_data)\n",
    "train_data=pd.DataFrame(pca.transform(x_data))\n",
    "#train_data=pd.concat([train_data, x_data[corr_inc_index]], axis=1)\n",
    "#print(train_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(train_data.head(5))\n",
    "#print(train_y.head(5))\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val,  y_train, y_val = train_test_split(train_data, train_y['교통량'], test_size=0.2, random_state=42)\n",
    "#print(x_train)\n",
    "#print(y_train)\n",
    "#print(len(x_val))\n",
    "#print(len(y_val))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear=LinearRegression()\n",
    "linear.fit(x_train, y_train)\n",
    "print(mean_squared_error(linear.predict(x_train),y_train))\n",
    "print(mean_squared_error(linear.predict(x_val),y_val))\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree=DecisionTreeRegressor()\n",
    "tree.fit(x_train, y_train)\n",
    "print(mean_squared_error(tree.predict(x_train), y_train))\n",
    "print(mean_squared_error(tree.predict(x_val), y_val))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf=RandomForestRegressor()\n",
    "rf.fit(x_train, y_train)\n",
    "print(mean_squared_error(rf.predict(x_train), y_train))\n",
    "print(mean_squared_error(rf.predict(x_val), y_val))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb=GradientBoostingRegressor()\n",
    "gb.fit(x_train, y_train)\n",
    "print(mean_squared_error(gb.predict(x_train), y_train))\n",
    "print(mean_squared_error(gb.predict(x_val), y_val))\n",
    "'''\n",
    "\n",
    "###하이퍼파라미터\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators':[3,10,30], 'max_features':[2,4,6,8]\n",
    "    },\n",
    "    {\n",
    "        'bootstrap':[False], 'n_estimators':[3,10], 'max_features':[2,3,4]\n",
    "    },\n",
    "]\n",
    "#grid_rf=GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "#grid_rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "grid_rf=RandomForestRegressor(max_features=8, n_estimators=30)\n",
    "grid_rf.fit(x_train, y_train)\n",
    "#print(grid_rf.best_params_)\n",
    "#print(grid_rf.best_score_)\n",
    "print(mean_squared_error(grid_rf.predict(x_train), y_train))\n",
    "print(mean_squared_error(grid_rf.predict(x_val), y_val))\n",
    "\n",
    "test_x_data=preprocessing(test_X)\n",
    "test_x_data.drop(corr_del_index, axis=1, inplace=True)\n",
    "test_data=pd.DataFrame(pca.transform(test_x_data))\n",
    "sub['교통량']=grid_rf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72059025 0.80798509 0.85381721 0.88174028 0.90344341 0.90710426\n",
      " 0.9107651  0.91442595 0.9180868  0.92174703 0.92540694 0.92906592\n",
      " 0.93272405 0.9363814  0.94003615 0.94369048 0.94734482 0.95099915\n",
      " 0.95465348 0.95830781 0.96196215 0.96561648 0.96927081 0.97292514\n",
      " 0.97657948 0.98022896 0.98386584 0.98749124 0.99111135 0.99394915\n",
      " 0.99615709 0.99744315 0.9985538  0.99937786 1.         1.\n",
      " 1.        ]\n",
      "110449.78903716497\n",
      "116318.78331261258\n",
      "0.0\n",
      "18559.6396985539\n",
      "1501.382937836535\n",
      "10832.31712436055\n",
      "15315.754085685541\n",
      "17551.906036366512\n",
      "1624.5008860439652\n",
      "10537.501906314672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "####데이터 읽어오기####\n",
    "#######################\n",
    "train_X = pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_trainX.csv\",encoding='euc-kr')\n",
    "train_y= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_trainy.csv\",encoding='euc-kr')\n",
    "test_X= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/Jeju_testX.csv\",encoding='euc-kr')\n",
    "sub= pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/jeju/main/subExample.csv\",encoding='euc-kr')\n",
    "#print(train_X.head(3))\n",
    "#print(train_y.head(3))\n",
    "#print(test_X.head(3))\n",
    "#print(sub.head(3))\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "####결측치수, 데이터타입, 연속형변수 분포 확인####\n",
    "##################################################\n",
    "#print(train_X.info())\n",
    "#print(test_X.info())\n",
    "#print(train_y.info())\n",
    "#print(sub.info())\n",
    "#print(train_X.describe())\n",
    "#print(test_X.describe())\n",
    "\n",
    "#corr_df=pd.merge(train_X, train_y, on='id')\n",
    "#corr_df=corr_df[['거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', \n",
    "#                 '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속','교통량']]\n",
    "#print(corr_df.corr()['교통량'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 이상치 기준\n",
    "total=pd.concat([train_X, test_X], axis=0)\n",
    "#print(total.describe())\n",
    "total_num=total[['거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', \n",
    "                 '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속']]\n",
    "total_str=total[['일자', '시도명', '읍면동명']]\n",
    "lower_outlier=list(map(lambda x: np.percentile(total_num[x],25)-((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "upper_outlier=list(map(lambda x: np.percentile(total_num[x],75)+((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "#print(lower_outlier)\n",
    "#print(upper_outlier)\n",
    "# 정규화\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robustscaler=RobustScaler()\n",
    "robustscaler.fit(total_num)\n",
    "\n",
    "\n",
    "# 범주형변수 train-test비교 - 차이없음\n",
    "#print(set(train_X['시도명'])-set(test_X['시도명']))\n",
    "#print(set(test_X['시도명'])-set(train_X['시도명']))\n",
    "#print(set(train_X['읍면동명'])-set(test_X['읍면동명']))\n",
    "#print(set(test_X['읍면동명'])-set(train_X['읍면동명']))\n",
    "#print(train_X['시도명'].value_counts())\n",
    "#print(test_X['시도명'].value_counts())\n",
    "#print(train_X['읍면동명'].value_counts())\n",
    "#print(test_X['읍면동명'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(xdata):\n",
    "    x_num=xdata[['거주인구', '근무인구', '방문인구', '총 유동인구', '평균 속도', \n",
    "                 '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속']]\n",
    "    x_str=xdata[['일자', '시도명', '읍면동명']]\n",
    "    \n",
    "    #for i in range(len(x_num.columns)):\n",
    "    #    x_num[x_num.columns[i]]=[lower_outlier[i] if x<lower_outlier[i] else upper_outlier[i] if x>upper_outlier[i] else x for x in x_num[x_num.columns[i]]]\n",
    "    #print(x_num.describe())\n",
    "    x_num=pd.DataFrame(robustscaler.transform(x_num), columns=x_num.columns)\n",
    "    #print(x_num.describe())\n",
    "    x_str=pd.get_dummies(x_str, columns=['시도명','읍면동명'])\n",
    "    x_str['일자']=pd.to_datetime(x_str['일자'])    \n",
    "    x_str['연도']=x_str['일자'].dt.year\n",
    "    x_str['월']=x_str['일자'].dt.month\n",
    "    x_str['일']=x_str['일자'].dt.day\n",
    "    x_str['weekend']=x_str['일자'].dt.weekday\n",
    "    x_str.drop('일자', axis=1, inplace=True)\n",
    "    #print(x_str)\n",
    "    x_data=pd.concat([x_num, x_str], axis=1)\n",
    "    #x_data.drop(['평균 속도', '평균 소요 시간', '평균 기온', '일강수량', '평균 풍속'], axis=1, inplace=True)\n",
    "    return x_data\n",
    "    \n",
    "x_data=preprocessing(train_X)\n",
    "corr_df=pd.concat([x_data, train_y], axis=1)\n",
    "corr_df=corr_df.drop('id', axis=1)\n",
    "#print(corr_df.corr()['교통량'].)\n",
    "corr_prep=corr_df.corr()['교통량']\n",
    "#print(corr_prep[abs(corr_prep)<0.1].index)\n",
    "#print(corr_prep[abs(corr_prep)>=0.3])\n",
    "corr_del_index=corr_prep[abs(corr_prep)<0.1].index\n",
    "x_data.drop(corr_del_index, axis=1, inplace=True)\n",
    "\n",
    "#corr_inc_index=['거주인구','근무인구','총 유동인구','평균 속도','시도명_서귀포시','시도명_제주시','읍면동명_노형동','읍면동명_도두동','읍면동명_화북동']\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "pca.fit(x_data)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "pca=PCA(n_components=18)\n",
    "pca.fit(x_data)\n",
    "train_data=pd.DataFrame(pca.transform(x_data))\n",
    "#train_data=pd.concat([train_data, x_data[corr_inc_index]], axis=1)\n",
    "#print(train_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(train_data.head(5))\n",
    "#print(train_y.head(5))\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val,  y_train, y_val = train_test_split(train_data, train_y['교통량'], test_size=0.2, random_state=42)\n",
    "#print(x_train)\n",
    "#print(y_train)\n",
    "#print(len(x_val))\n",
    "#print(len(y_val))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear=LinearRegression()\n",
    "linear.fit(x_train, y_train)\n",
    "print(mean_squared_error(linear.predict(x_train),y_train))\n",
    "print(mean_squared_error(linear.predict(x_val),y_val))\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree=DecisionTreeRegressor()\n",
    "tree.fit(x_train, y_train)\n",
    "print(mean_squared_error(tree.predict(x_train), y_train))\n",
    "print(mean_squared_error(tree.predict(x_val), y_val))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf=RandomForestRegressor()\n",
    "rf.fit(x_train, y_train)\n",
    "print(mean_squared_error(rf.predict(x_train), y_train))\n",
    "print(mean_squared_error(rf.predict(x_val), y_val))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb=GradientBoostingRegressor()\n",
    "gb.fit(x_train, y_train)\n",
    "print(mean_squared_error(gb.predict(x_train), y_train))\n",
    "print(mean_squared_error(gb.predict(x_val), y_val))\n",
    "\n",
    "\n",
    "###하이퍼파라미터\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators':[3,10,30], 'max_features':[2,4,6,8]\n",
    "    },\n",
    "    {\n",
    "        'bootstrap':[False], 'n_estimators':[3,10], 'max_features':[2,3,4]\n",
    "    },\n",
    "]\n",
    "#grid_rf=GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "#grid_rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "grid_rf=RandomForestRegressor(max_features=8, n_estimators=30)\n",
    "grid_rf.fit(x_train, y_train)\n",
    "#print(grid_rf.best_params_)\n",
    "#print(grid_rf.best_score_)\n",
    "print(mean_squared_error(grid_rf.predict(x_train), y_train))\n",
    "print(mean_squared_error(grid_rf.predict(x_val), y_val))\n",
    "\n",
    "test_x_data=preprocessing(test_X)\n",
    "test_x_data.drop(corr_del_index, axis=1, inplace=True)\n",
    "test_data=pd.DataFrame(pca.transform(test_x_data))\n",
    "sub['교통량']=grid_rf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
