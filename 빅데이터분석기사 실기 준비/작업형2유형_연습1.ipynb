{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-2e993848b480>:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  total_str['주구매상품']=['기타' if x=='소형가전' else x for x in total_str['주구매상품']]\n",
      "<ipython-input-105-2e993848b480>:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n",
      "<ipython-input-105-2e993848b480>:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_str['주구매상품']=['기타' if x=='소형가전' else x for x in data_str['주구매상품']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6952657797791166\n",
      "0.6891250825677055\n",
      "0.6084789527412479\n",
      "0       0.483976\n",
      "1       0.247108\n",
      "2       0.230825\n",
      "3       0.453672\n",
      "4       0.491549\n",
      "          ...   \n",
      "2477    0.527290\n",
      "2478    0.440954\n",
      "2479    0.543332\n",
      "2480    0.409989\n",
      "2481    0.450810\n",
      "Name: 1, Length: 2482, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-2e993848b480>:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n",
      "<ipython-input-105-2e993848b480>:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_str['주구매상품']=['기타' if x=='소형가전' else x for x in data_str['주구매상품']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "y_train = pd.read_csv('https://raw.githubusercontent.com/Datamanim/dataq/main/y_train.csv')\n",
    "X_train = pd.read_csv('https://raw.githubusercontent.com/Datamanim/dataq/main/X_train.csv',encoding='euc-kr')\n",
    "test  = pd.read_csv('https://raw.githubusercontent.com/Datamanim/dataq/main/X_test.csv',encoding='euc-kr')\n",
    "#print(X_train.head(5))\n",
    "#print(y_train.head(5))\n",
    "#print(test.head(5))\n",
    "#print(X_train.info())\n",
    "#print(y_train.info())\n",
    "#print(test.info())\n",
    "#환불금액 컬럼 널값 있음\n",
    "#print(X_train[X_train['총구매액']<0])\n",
    "#print(X_train[X_train['최대구매액']<0])\n",
    "#print(test[test['총구매액']<0])\n",
    "#print(X_train[X_train['총구매액']<X_train['최대구매액']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#스케일링 및 아웃라이어 구하기 위한 전체\n",
    "total=pd.concat([X_train,test], axis=0)\n",
    "total['내점구매건수']=total['내점일수']*total['내점당구매건수']\n",
    "total['환불더한총구매액']=total['총구매액']*total['환불금액']\n",
    "rscaler=RobustScaler()\n",
    "#total_num=total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "rscaler.fit(total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]=rscaler.transform(total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "total_num=total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "outlier=list(map(lambda x: np.percentile(total_num[x],75)+((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "total_str=total[['주구매상품', '주구매지점']]\n",
    "total_str['주구매상품']=['기타' if x=='소형가전' else x for x in total_str['주구매상품']]\n",
    "total_str=pd.get_dummies(total_str, columns=['주구매상품', '주구매지점'])\n",
    "sscaler=StandardScaler()\n",
    "sscaler.fit(total_str)\n",
    "#pd.DataFrame(sscaler.transform(total_str), columns=total_str.columns)\n",
    "# 전처리 확인용\n",
    "#print(set(X_train['주구매상품'])-set(test['주구매상품']))\n",
    "#print(set(test['주구매상품'])-set(X_train['주구매상품']))\n",
    "#print(set(X_train['주구매지점'])-set(test['주구매지점']))\n",
    "#print(set(test['주구매지점'])-set(X_train['주구매지점']))\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    data=data.fillna(0)\n",
    "    data['환불더한총구매액']=data['총구매액']*data['환불금액']\n",
    "    data['내점구매건수']=data['내점일수']*data['내점당구매건수']\n",
    "    data['최대구매액']=[-x if x<0 else x for x in data['최대구매액']]\n",
    "    data['총구매액']=[0 if x<0 else x for x in data['총구매액']]\n",
    "    data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]=rscaler.transform(data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "    data_num=data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "    for i in range(len(data_num.columns)):\n",
    "        data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n",
    "    data_str=data[['주구매상품', '주구매지점']]\n",
    "    data_str['주구매상품']=['기타' if x=='소형가전' else x for x in data_str['주구매상품']]\n",
    "    data_str=pd.get_dummies(data_str, columns=['주구매상품', '주구매지점'])\n",
    "    data_str=pd.DataFrame(sscaler.transform(data_str), columns=data_str.columns)\n",
    "    \n",
    "    data=pd.concat([data_num, data_str, data['cust_id']], axis=1)\n",
    "    return(data)\n",
    "\n",
    "X_train=preprocessing(X_train)\n",
    "x=pd.merge(X_train, y_train, on='cust_id')\n",
    "corr_gender=x.corr()['gender']\n",
    "#print(corr_gender[abs(corr_gender)>0.05])\n",
    "corr_del_index=corr_gender[abs(corr_gender)<0.047].index\n",
    "X_train_test=X_train.drop(corr_del_index, axis=1)\n",
    "pca=PCA()\n",
    "pca.fit(X_train_test)\n",
    "#print(pca.explained_variance_ratio_.cumsum())\n",
    "pca_num=13\n",
    "pca=PCA(n_components=pca_num)\n",
    "pca.fit(X_train_test)\n",
    "\n",
    "def select_val(data):\n",
    "    data.drop(corr_del_index, axis=1, inplace=True)\n",
    "    data=pd.DataFrame(pca.transform(data))\n",
    "    return data\n",
    "\n",
    "X_train=select_val(X_train)\n",
    "#print(y_train['gender'].value_counts())\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "#print(len(x_train))\n",
    "#print(len(y_train))\n",
    "#print(len(x_val))\n",
    "#print(len(y_val))\n",
    "y_train=y_train['gender']\n",
    "y_val=y_val['gender']\n",
    "#print(y_train.value_counts())\n",
    "#print(y_val.value_counts())\n",
    "\n",
    "'''\n",
    "glm=LogisticRegression()\n",
    "glm.fit(x_train, y_train)\n",
    "print(glm.score(x_train, y_train))\n",
    "print(glm.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, glm.predict(x_val)))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "print(rf.score(x_train, y_train))\n",
    "print(rf.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, rf.predict(x_val)))\n",
    "\n",
    "gb=GradientBoostingClassifier()\n",
    "gb.fit(x_train, y_train)\n",
    "print(gb.score(x_train, y_train))\n",
    "print(gb.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, gb.predict(x_val)))\n",
    "'''\n",
    "\n",
    "param={ \"n_estimators\": range(50, 100, 25), \n",
    "       \"max_depth\": [1, 2, 4], \n",
    "       \"learning_rate\": [0.001, 0.01, 0.1], \n",
    "       \"max_features\": [3,4,5,6], }\n",
    "grid=GridSearchCV(GradientBoostingClassifier(random_state=42), param, cv=6, scoring=\"roc_auc\", n_jobs=-1,)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "#grid=GradientBoostingClassifier(learning_rate=0.1, max_depth=2, max_features=4, n_estimators=50, random_state=42)\n",
    "#grid.fit(x_train, y_train)\n",
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)\n",
    "print(grid.score(x_train, y_train))\n",
    "print(grid.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, grid.predict(x_val)))\n",
    "\n",
    "\n",
    "test=preprocessing(test)\n",
    "test=select_val(test)\n",
    "print(pd.DataFrame(grid.predict_proba(test))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6084789527412479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-112-3e2bdbf08d86>:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n",
      "C:\\ai\\programs\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'learning_rate': 0.15, 'max_depth': 1, 'n_estimators': 125}\n",
      "0.6714802895912912\n",
      "0.7108318554037485\n",
      "0.6913528091421939\n",
      "0.5989920714169888\n",
      "0       0.549848\n",
      "1       0.209512\n",
      "2       0.177712\n",
      "3       0.470841\n",
      "4       0.484751\n",
      "          ...   \n",
      "2477    0.603446\n",
      "2478    0.501754\n",
      "2479    0.605285\n",
      "2480    0.400077\n",
      "2481    0.509953\n",
      "Name: 1, Length: 2482, dtype: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-112-3e2bdbf08d86>:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "y_train = pd.read_csv('https://raw.githubusercontent.com/Datamanim/dataq/main/y_train.csv')\n",
    "X_train = pd.read_csv('https://raw.githubusercontent.com/Datamanim/dataq/main/X_train.csv',encoding='euc-kr')\n",
    "test  = pd.read_csv('https://raw.githubusercontent.com/Datamanim/dataq/main/X_test.csv',encoding='euc-kr')\n",
    "#print(X_train.head(5))\n",
    "#print(y_train.head(5))\n",
    "#print(test.head(5))\n",
    "#print(X_train.info())\n",
    "#print(y_train.info())\n",
    "#print(test.info())\n",
    "#환불금액 컬럼 널값 있음\n",
    "#print(X_train[X_train['총구매액']<0])\n",
    "#print(X_train[X_train['최대구매액']<0])\n",
    "#print(test[test['총구매액']<0])\n",
    "#print(X_train[X_train['총구매액']<X_train['최대구매액']])\n",
    "\n",
    "X_train=X_train.drop([1521, 2035], axis=0)\n",
    "y_train=y_train.drop([1521, 2035], axis=0)\n",
    "X_train=X_train.reset_index().drop('index', axis=1)\n",
    "y_train=y_train.reset_index().drop('index', axis=1)\n",
    "\n",
    "#스케일링 및 아웃라이어 구하기 위한 전체\n",
    "total=pd.concat([X_train,test], axis=0)\n",
    "total['내점구매건수']=total['내점일수']*total['내점당구매건수']\n",
    "total['환불더한총구매액']=total['총구매액']*total['환불금액']\n",
    "rscaler=RobustScaler()\n",
    "#total_num=total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "rscaler.fit(total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]=rscaler.transform(total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "total_num=total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "outlier=list(map(lambda x: np.percentile(total_num[x],75)+((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "total_str=total[['주구매상품', '주구매지점']]\n",
    "#print(total_str[total_str['주구매상품']=='소형가전'])\n",
    "#total_str['주구매상품']=['기타' if x=='소형가전' else x for x in total_str['주구매상품']]\n",
    "total_str=pd.get_dummies(total_str, columns=['주구매상품', '주구매지점'])\n",
    "sscaler=StandardScaler()\n",
    "sscaler.fit(total_str)\n",
    "#pd.DataFrame(sscaler.transform(total_str), columns=total_str.columns)\n",
    "# 전처리 확인용\n",
    "#print(set(X_train['주구매상품'])-set(test['주구매상품']))\n",
    "#print(set(test['주구매상품'])-set(X_train['주구매상품']))\n",
    "#print(set(X_train['주구매지점'])-set(test['주구매지점']))\n",
    "#print(set(test['주구매지점'])-set(X_train['주구매지점']))\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    data=data.fillna(0)\n",
    "    data['환불더한총구매액']=data['총구매액']*data['환불금액']\n",
    "    data['내점구매건수']=data['내점일수']*data['내점당구매건수']\n",
    "    data['최대구매액']=[-x if x<0 else x for x in data['최대구매액']]\n",
    "    data['총구매액']=[0 if x<0 else x for x in data['총구매액']]\n",
    "    data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]=rscaler.transform(data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "    data_num=data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "    for i in range(len(data_num.columns)):\n",
    "        data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n",
    "    data_str=data[['주구매상품', '주구매지점']]\n",
    "    \n",
    "    #data_str['주구매상품']=['기타' if x=='소형가전' else x for x in data_str['주구매상품']]\n",
    "    data_str=pd.get_dummies(data_str, columns=['주구매상품', '주구매지점'])\n",
    "    data_str=pd.DataFrame(sscaler.transform(data_str), columns=data_str.columns)\n",
    "    \n",
    "    data=pd.concat([data_num, data_str, data['cust_id']], axis=1)\n",
    "    return(data)\n",
    "\n",
    "X_train=preprocessing(X_train)\n",
    "x=pd.merge(X_train, y_train, on='cust_id')\n",
    "corr_gender=x.corr()['gender']\n",
    "#print(corr_gender[abs(corr_gender)>0.05])\n",
    "corr_del_index=corr_gender[abs(corr_gender)<0.047].index\n",
    "X_train_test=X_train.drop(corr_del_index, axis=1)\n",
    "pca=PCA()\n",
    "pca.fit(X_train_test)\n",
    "#print(pca.explained_variance_ratio_.cumsum())\n",
    "pca_num=13\n",
    "pca=PCA(n_components=pca_num)\n",
    "pca.fit(X_train_test)\n",
    "\n",
    "def select_val(data):\n",
    "    data.drop(corr_del_index, axis=1, inplace=True)\n",
    "    data=pd.DataFrame(pca.transform(data))\n",
    "    return data\n",
    "\n",
    "X_train=select_val(X_train)\n",
    "#print(y_train['gender'].value_counts())\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "#print(len(x_train))\n",
    "#print(len(y_train))\n",
    "#print(len(x_val))\n",
    "#print(len(y_val))\n",
    "y_train=y_train['gender']\n",
    "y_val=y_val['gender']\n",
    "#print(y_train.value_counts())\n",
    "#print(y_val.value_counts())\n",
    "\n",
    "'''\n",
    "glm=LogisticRegression()\n",
    "glm.fit(x_train, y_train)\n",
    "print(glm.score(x_train, y_train))\n",
    "print(glm.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, glm.predict(x_val)))\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "print(rf.score(x_train, y_train))\n",
    "print(rf.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, rf.predict(x_val)))\n",
    "\n",
    "gb=GradientBoostingClassifier()\n",
    "gb.fit(x_train, y_train)\n",
    "print(gb.score(x_train, y_train))\n",
    "print(gb.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, gb.predict(x_val)))\n",
    "'''\n",
    "'''\n",
    "param={ \"n_estimators\": range(50, 100, 25), \n",
    "       \"max_depth\": [1, 2, 4], \n",
    "       \"learning_rate\": [0.001, 0.01, 0.1], \n",
    "       \"max_features\": [3,4,5,6], }\n",
    "grid=GridSearchCV(GradientBoostingClassifier(random_state=42), param, cv=6, scoring=\"roc_auc\", n_jobs=4,)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "#grid=GradientBoostingClassifier(learning_rate=0.1, max_depth=2, max_features=4, n_estimators=50, random_state=42)\n",
    "#grid.fit(x_train, y_train)\n",
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)\n",
    "print(grid.score(x_train, y_train))\n",
    "print(grid.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, grid.predict(x_val)))\n",
    "'''\n",
    "\n",
    "'''\n",
    "xgb=XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "print(xgb.score(x_train, y_train))\n",
    "print(xgb.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, xgb.predict(x_val)))\n",
    "'''\n",
    "param={ \"n_estimators\": [75, 100, 125], \n",
    "       \"max_depth\": [1, 2, 4], \n",
    "       \"learning_rate\": [0.01, 0.1, 0.125, 0.15], \n",
    "}\n",
    "grid=GridSearchCV(XGBClassifier(random_state=42), param, cv=6, scoring=\"roc_auc\", n_jobs=-1,)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "#grid=GradientBoostingClassifier(learning_rate=0.1, max_depth=2, max_features=4, n_estimators=50, random_state=42)\n",
    "#grid.fit(x_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.score(x_train, y_train))\n",
    "print(grid.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, grid.predict(x_val)))\n",
    "\n",
    "test=preprocessing(test)\n",
    "test=select_val(test)\n",
    "print(pd.DataFrame(grid.predict_proba(test))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "X_train = pd.read_csv('data/X_train.csv')\n",
    "test  = pd.read_csv('data/X_test.csv')\n",
    "test_index=test['cust_id']\n",
    "\n",
    "X_train=X_train.drop([1521, 2035], axis=0)\n",
    "y_train=y_train.drop([1521, 2035], axis=0)\n",
    "X_train=X_train.reset_index().drop('index', axis=1)\n",
    "y_train=y_train.reset_index().drop('index', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#스케일링 및 아웃라이어 구하기 위한 전체\n",
    "total=pd.concat([X_train,test], axis=0)\n",
    "total['내점구매건수']=total['내점일수']*total['내점당구매건수']\n",
    "total['환불더한총구매액']=total['총구매액']*total['환불금액']\n",
    "rscaler=RobustScaler()\n",
    "#total_num=total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "rscaler.fit(total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]=rscaler.transform(total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "total_num=total[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "outlier=list(map(lambda x: np.percentile(total_num[x],75)+((np.percentile(total_num[x],75)-np.percentile(total_num[x],25))*1.5) ,total_num.columns))\n",
    "total_str=total[['주구매상품', '주구매지점']]\n",
    "#total_str['주구매상품']=['기타' if x=='소형가전' else x for x in total_str['주구매상품']]\n",
    "total_str=pd.get_dummies(total_str, columns=['주구매상품', '주구매지점'])\n",
    "sscaler=StandardScaler()\n",
    "sscaler.fit(total_str)\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    data=data.fillna(0)\n",
    "    data['환불더한총구매액']=data['총구매액']*data['환불금액']\n",
    "    data['내점구매건수']=data['내점일수']*data['내점당구매건수']\n",
    "    data['최대구매액']=[-x if x<0 else x for x in data['최대구매액']]\n",
    "    data['총구매액']=[0 if x<0 else x for x in data['총구매액']]\n",
    "    data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]=rscaler.transform(data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']])\n",
    "    data_num=data[['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '내점구매건수', '환불더한총구매액']]\n",
    "    #for i in range(len(data_num.columns)):\n",
    "    #    data_num[data_num.columns[i]]=[outlier[i] if x>outlier[i] else x for x in data_num[data_num.columns[i]]]\n",
    "    data_str=data[['주구매상품', '주구매지점']]\n",
    "    #data_str['주구매상품']=['기타' if x=='소형가전' else x for x in data_str['주구매상품']]\n",
    "    data_str=pd.get_dummies(data_str, columns=['주구매상품', '주구매지점'])\n",
    "    data_str=pd.DataFrame(sscaler.transform(data_str), columns=data_str.columns)\n",
    "    \n",
    "    data=pd.concat([data_num, data_str, data['cust_id']], axis=1)\n",
    "    return(data)\n",
    "\n",
    "X_train=preprocessing(X_train)\n",
    "x=pd.merge(X_train, y_train, on='cust_id')\n",
    "corr_gender=x.corr()['gender']\n",
    "#print(corr_gender[abs(corr_gender)>0.05])\n",
    "corr_del_index=corr_gender[abs(corr_gender)<0.047].index\n",
    "X_train_test=X_train.drop(corr_del_index, axis=1)\n",
    "#pca=PCA()\n",
    "#pca.fit(X_train_test)\n",
    "#print(pca.explained_variance_ratio_.cumsum())\n",
    "pca_num=13\n",
    "pca=PCA(n_components=pca_num)\n",
    "pca.fit(X_train_test)\n",
    "\n",
    "def select_val(data):\n",
    "    data.drop(corr_del_index, axis=1, inplace=True)\n",
    "    data=pd.DataFrame(pca.transform(data))\n",
    "    return data\n",
    "\n",
    "X_train=select_val(X_train)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train=y_train['gender']\n",
    "y_val=y_val['gender']\n",
    "\n",
    "\n",
    "\n",
    "param={ \"n_estimators\": range(50, 100, 25), \n",
    "       \"max_depth\": [1, 2, 4], \n",
    "       \"learning_rate\": [0.01, 0.1], \n",
    " }\n",
    "grid=GridSearchCV(GradientBoostingClassifier(random_state=42), param, cv=6, scoring=\"roc_auc\", n_jobs=-1,)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(grid.score(x_train, y_train))\n",
    "print(grid.score(x_val, y_val))\n",
    "print(roc_auc_score(y_val, grid.predict(x_val)))\n",
    "\n",
    "\n",
    "test=preprocessing(test)\n",
    "test=select_val(test)\n",
    "\n",
    "rst=pd.DataFrame({'custid':test_index,\n",
    "   'gender':pd.DataFrame(grid.predict_proba(test))[1]})\n",
    "rst.to_csv(\"0000.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
